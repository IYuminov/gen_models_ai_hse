{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUgNFOSlFEzm"
      },
      "source": [
        "# –ú–û–∏–í–° \"–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏\", 5-–π –º–æ–¥—É–ª—å\n",
        "\n",
        "# Homework 1\n",
        "\n",
        "–í —ç—Ç–æ–π –¥–æ–º–∞—à–Ω–µ–π —Ä–∞–±–æ—Ç–µ –≤–∞–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç –¥–æ–±–∞–≤–∏—Ç—å –∫ BERT'—É –¥–µ–∫–æ–¥–µ—Ä–Ω—É—é —á–∞—Å—Ç—å –∏ —Ä–µ—à–∏—Ç—å –∑–∞–¥–∞—á—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–π –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.\n",
        "\n",
        "–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∫ —ç—Ç–æ–º—É –Ω–∞ –æ—Ç–ª–∏—á–Ω—É—é –æ—Ü–µ–Ω–∫—É –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–æ–¥—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –º–µ–Ω–µ–µ –∂–∞–¥–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –≤—ã–±–æ—Ä–∞ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.\n",
        "\n",
        "*–ú—ã —Å—Ä–∞–∑—É –≤–∞—Å –ø—Ä–µ–¥–æ—Å—Ç–µ—Ä–µ–≥–∞–µ–º –ø–æ–ø–∞—Å—Ç—å –≤ –ø–µ—Ç–ª—é –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–≥–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. –≠—Ç–∞ –¥–æ–º–∞—à–∫–∞ –Ω–µ –Ω–∞ –ø—Ä–æ–±–∏—Ç–∏–µ —Å–∫–æ—Ä–∞. –ú—ã –±—É–¥–µ–º –ø—Ä–æ–≤–µ—Ä—è—Ç—å, —á—Ç–æ –≤—ã, –≤ —Ü–µ–ª–æ–º, —Å–¥–µ–ª–∞–ª–∏ –≤—Å–µ –≤–µ—Ä–Ω–æ –∏ —Å–º–æ–≥–ª–∏ –ø–æ–ª—É—á–∏—Ç—å –∫–∞–∫—É—é-—Ç–æ –±–æ–ª–µ–µ-–º–µ–Ω–µ–µ –∞–¥–µ–∫–≤–∞—Ç–Ω—É—é (—Ç–∞–∫—É—é, –∫–æ—Ç–æ—Ä–∞—è –∑–∞–º–µ—Ç–Ω–æ –ª—É—á—à–µ —Ç–æ–π, —á—Ç–æ –±—ã–ª–∞ –¥–æ –Ω–∞—á–∞–ª–∞ –æ–±—É—á–µ–Ω–∏—è) –≥–µ–Ω–µ—Ä–∞—Ü–∏—é. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –µ—Å–ª–∏ –≤—ã –≤–∏–¥–∏—Ç–µ, —á—Ç–æ –º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è, –Ω–µ –Ω–∞–¥–æ –¥–æ–æ–±—É—á–∞—Ç—å –µ—ë —Å—É—Ç–∫–∞–º–∏. –ù–µ—Å–∫–æ–ª—å–∫–∏—Ö —á–∞—Å–æ–≤ —Ç–æ—á–Ω–æ –¥–æ–ª–∂–Ω–æ —Ö–≤–∞—Ç–∏—Ç—å.*\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "–ü–æ –ª—é–±—ã–º –≤–æ–ø—Ä–æ—Å–∞–º –∫–∞—Å–∞—Ç–µ–ª—å–Ω–æ —ç—Ç–æ–π –¥–æ–º–∞—à–Ω–µ–π —Ä–∞–±–æ—Ç—ã –æ–±—Ä–∞—â–∞–π—Ç–µ—Å—å –∫–æ —Å–≤–æ–∏–º –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞–º\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-oW4ttVEL_9"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "python3 -m pip install transformers datasets evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ygnbZcjlgJR9"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import warnings\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import evaluate\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "from IPython.display import clear_output\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizer, BertModel, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYW38mH0gKX0"
      },
      "source": [
        "## –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö (0.5 –±–∞–ª–ª–∞)\n",
        "\n",
        "–ú—ã –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è –¥–∞—Ç–∞—Å–µ—Ç–æ–º —Å ü§ó –ò–ª—å–∏ –ì—É—Å–µ–≤–∞ \"gazeta\". –û–Ω –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø–∞—Ä—ã (–ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏ -- –µ–≥–æ —Å–∞–º–º–∞—Ä–∏). –ü–∞—Ä—ã –±—ã–ª–∏ –≤–∑—è—Ç—ã —Å –æ–¥–Ω–æ–∏–º–µ–Ω–Ω–æ–≥–æ —Å–∞–π—Ç–∞ –≤ –¥–æ–º–µ–Ω–µ .ru\n",
        "\n",
        "–ë–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–æ –ø—Ä–æ –¥–∞—Ç–∞—Å–µ—Ç –º–æ–∂–Ω–æ –ø—Ä–æ—á–∏—Ç–∞—Ç—å [–∑–¥–µ—Å—å](https://huggingface.co/datasets/IlyaGusev/gazeta)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mDV4tJzzB5Hi"
      },
      "outputs": [],
      "source": [
        "# –ó–∞–≥—Ä—É–∑–∏–º –¥–∞–Ω–Ω—ã–µ —Å –ø–æ–ø–æ—â—å—é –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ datasets\n",
        "\n",
        "dataset = load_dataset('IlyaGusev/gazeta', revision=\"v2.0\", split='train[:5%]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOjri9a4h6K6"
      },
      "source": [
        "–í—ã –¥–æ–ª–∂–Ω—ã –ø–æ–º–Ω–∏—Ç—å, —á—Ç–æ —Ç–µ–∫—Å—Ç—ã –ø–µ—Ä–µ–¥ –ø–æ–¥–∞—á–µ–π –≤ –º–æ–¥–µ–ª—å –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ **—Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å**.\n",
        "\n",
        "–î–æ–±–∞–≤—å—Ç–µ –ø–∞–¥–¥–∏–Ω–≥ –¥–æ `max_length=512` –¥–ª—è –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –∞ —Ç–∞–∫–∂–µ –¥–æ `max_length=128` –¥–ª—è –º–µ—Ç–æ–∫.\n",
        "\n",
        "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –æ–±—Ä–µ–∑–∫—É —Ç–µ–∫—Å—Ç–æ–≤, –¥–ª–∏–Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –≤ —Ç–æ–∫–µ–Ω–∞—Ö –ø—Ä–µ–≤—ã—à–∞–µ—Ç `max_length`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yp19tTXfgHsq"
      },
      "outputs": [],
      "source": [
        "# –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–∏ Bert\n",
        "\n",
        "model_name = 'deepvk/bert-base-uncased' # –£–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ BERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def preprocess(examples):\n",
        "\n",
        "    # —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä –¥–ª—è —Ç–µ–∫—Å—Ç–∞\n",
        "    model_inputs = tokenizer(\n",
        "        examples['text'], \n",
        "        max_length= 512,\n",
        "        truncation= True,\n",
        "        padding= 'max_length'\n",
        "        )\n",
        "    # —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä –¥–ª—è –º–µ—Ç–æ–∫\n",
        "    labels = tokenizer(\n",
        "        examples['summary'],\n",
        "        max_length= 128,\n",
        "        truncation= True,\n",
        "        padding= 'max_length' \n",
        "        )\n",
        "    \n",
        "    model_inputs['labels'] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a8e5203a02b845a29f57ca545c50c5d0",
            "09b11d6c6eae4979a1c8cd42e32730ae",
            "911fd70fda12476ab2b788433d6ff83f",
            "bcfc58a3f90745449c3f559aa5ab999a",
            "40c29f2dde174a3c8442d9b86a4e3fe9",
            "ab3d20b0e457431dbe24120bb4becede",
            "68b44cb7bddd4a2f950db1a9c00ce066",
            "2665d63c206a45d88fada74bc984771e",
            "c0ddd3783ec047569c2024e461d5ad0f",
            "06437165ba564bff9e29aa53f4c0df5b",
            "665428d410664f94babcd067b879ce2e"
          ]
        },
        "id": "VQxpZ5ivhjlh",
        "outputId": "b3876676-3dc7-4d1d-894e-f0630172afa4"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = dataset.map(preprocess, batched=False)\n",
        "tokenized_dataset.set_format('torch')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXQ8gq1UijNj"
      },
      "source": [
        "–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ —Å–æ–≤–µ—Ç—É–µ–º –ø–æ–¥–±–∏—Ä–∞—Ç—å —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —á—Ç–æ–± —É—Ç–∏–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –º–∞–∫—Å–∏–º—É–º –¥–æ—Å—Ç—É–ø–Ω–æ–π VRAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xmMCjFAqSDWR"
      },
      "outputs": [],
      "source": [
        "split_data = tokenized_dataset.train_test_split(test_size = 0.2)\n",
        "\n",
        "train_dataloader = DataLoader(split_data['train'], batch_size= 8, shuffle= True)\n",
        "eval_dataloader = DataLoader(split_data['test'], batch_size= 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0J1iEfFHxRz"
      },
      "source": [
        "## –†–µ–∞–ª–∏–∑–∞—Ü–∏—è Decoder-c–µ—Ç–∏ (3 –±–∞–ª–ª–∞)\n",
        "\n",
        "–í –¥–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ –≤–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ **—Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –¥–µ–∫–æ–¥–µ—Ä –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞**.\n",
        "\n",
        "–ú–æ–∂–µ—Ç–µ –≤–¥–æ—Ö–Ω–æ–≤–ª—è—Ç—å—Å—è –∫–æ–¥–æ–º —Å —Å–µ–º–∏–Ω–∞—Ä–∞ 1 –ø–æ GPT. –í –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–µ—Å–æ–≤ —Å—Ç–æ–∏—Ç (–Ω–æ –Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ) –ø—Ä–æ—è–≤–∏—Ç—å —Å–º–µ–∫–∞–ª–∫—É"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "y5qSblF1EMEV"
      },
      "outputs": [],
      "source": [
        "# –ö–ª–∞—Å—Å –º–æ–¥–µ–ª–∏ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ BERT —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º –¥–µ–∫–æ–¥–µ—Ä–æ–º\n",
        "\n",
        "class BertSummarizer(nn.Module):\n",
        "    def __init__(self, bert_model_name='bert-base-uncased', hidden_size=768, num_decoder_layers=3, num_heads=8):\n",
        "        super(BertSummarizer, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –≤—Ö–æ–¥–µ –≤ –¥–µ–∫–æ–¥–µ—Ä\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings = self.bert.config.vocab_size,\n",
        "            embedding_dim = self.hidden_size\n",
        "            )\n",
        "\n",
        "        self.decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model = self.hidden_size,\n",
        "            nhead = num_heads\n",
        "            )\n",
        "        \n",
        "        self.decoder = nn.TransformerDecoder(\n",
        "            decoder_layer = self.decoder_layer,\n",
        "            num_layers = num_decoder_layers\n",
        "            )\n",
        "        \n",
        "        self.fc_out = nn.Linear(self.hidden_size, self.bert.config.vocab_size)\n",
        "        self.softmax = nn.Softmax()\n",
        "        \n",
        "\n",
        "    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–∞—Å–∫–∏ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∑–∞–≥–ª—è–¥—ã–≤–∞–Ω–∏—è –≤–ø–µ—Ä–µ–¥ –≤ –¥–µ–∫–æ–¥–µ—Ä–µ\n",
        "    def generate_square_subsequent_mask(self, T):\n",
        "    \n",
        "        result = torch.zeros(T, T)\n",
        "        mask = torch.triu(torch.ones(T, T), diagonal=1)\n",
        "        result[mask.bool()] = float('-inf')\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, decoder_input_ids):\n",
        "        encoder_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        memory = encoder_outputs.last_hidden_state  # –í—ã—Ö–æ–¥—ã BERT –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –¥–µ–∫–æ–¥–µ—Ä–µ\n",
        "\n",
        "        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–µ–∫–æ–¥–µ—Ä–∞\n",
        "        embedded = self.embedding(decoder_input_ids)\n",
        "\n",
        "        tgt_mask = self.generate_square_subsequent_mask(embedded.size(1))#.to(input_ids.device)\n",
        "        decoded = self.decoder(tgt=embedded.transpose(0, 1), memory = memory.transpose(0, 1), tgt_mask= tgt_mask)\n",
        "        output = self.fc_out(decoded.transpose(0, 1))\n",
        "\n",
        "        return self.softmax(output)\n",
        "    \n",
        "\n",
        "    def generate(self, input_ids, attention_mask, tokenizer, max_len= 50):\n",
        "        encoder_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        memory = encoder_outputs.last_hidden_state\n",
        "        batch_size = input_ids.size(0)\n",
        "\n",
        "        # –ù–∞—á–∏–Ω–∞–µ–º —Å —Ç–æ–∫–µ–Ω–∞ [CLS] –∏–ª–∏ [BOS] (–Ω–∞—á–∞–ª–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏)\n",
        "        decoder_input_ids = torch.full((batch_size, 1), tokenizer.cls_token_id, dtype=torch.long).to(input_ids.device)\n",
        "        memory = memory.transpose(0, 1)\n",
        "        # generated_tokens = []\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            embedded = self.embedding(decoder_input_ids).transpose(0, 1)\n",
        "\n",
        "            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–∞—Å–∫–∏ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∑–∞–≥–ª—è–¥—ã–≤–∞–Ω–∏—è –≤–ø–µ—Ä–µ–¥\n",
        "            decoder_attention_mask = self.generate_square_subsequent_mask(embedded.size(0)).to(input_ids.device)\n",
        "            decoder_output = self.decoder(tgt=embedded, memory=memory, tgt_mask=decoder_attention_mask)\n",
        "\n",
        "            output = self.fc_out(decoder_output.transpose(0, 1))\n",
        "\n",
        "            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å —Ç–æ–∫–µ–Ω–∞ —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é.\n",
        "            # –ü–æ–º–Ω–∏—Ç–µ, –µ—Å–ª–∏ EOS –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω, –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é\n",
        "\n",
        "            gen_token = torch.argmax(output[:, -1, :], dim=-1, keepdim=True)\n",
        "            decoder_input_ids = torch.cat([decoder_input_ids, gen_token], dim=-1)\n",
        "            \n",
        "            if tokenizer.sep_token_id in gen_token:\n",
        "                break\n",
        "\n",
        "        generated_sequence = tokenizer.decode(\n",
        "            decoder_input_ids.squeeze().tolist(),\n",
        "            skip_special_tokens=True\n",
        "            )\n",
        "\n",
        "        return generated_sequence\n",
        "    \n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=50):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "    \n",
        "\n",
        "\n",
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–∞—à—É –º–æ–¥–µ–ª—å –∏ –ø–æ—Å–º–æ—Ä–∏–º –Ω–∞ –µ–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—Ä—É—Ä—É\n",
        "model = BertSummarizer(bert_model_name=model_name)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "Z5VXXCKgecHc"
      },
      "outputs": [],
      "source": [
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–∞—à—É –º–æ–¥–µ–ª—å –∏ –ø–æ—Å–º–æ—Ä–∏–º –Ω–∞ –µ–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—Ä—É—Ä—É\n",
        "\n",
        "# model = BertSummarizer(bert_model_name=model_name)\n",
        "# model = model.to(device)\n",
        "# model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "TtvZWsojOh6g",
        "outputId": "6c323397-d141-4169-a3e8-c278b8ddf8fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'–∞–ª–µ–∫—Å–µ–∏ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–∞—è —ç–∫—Å–ø —é–Ω–æ—à–∏ –≤—Å–ø –≤—Å—Ç–∞–ª–∏ –∑–∞–ø–∞–¥—É –æ—Ä–≥–∞–Ω–∏–∑ ##–∏—Ç–µ–ª—å–Ω—ã—Ö ##–æ–æ —Ñ–µ–∏—Å –ø—Ä–æ–¥–∞–≤–µ—Ü –µ–≥–µ—Ä –ø—Ä–∞–≤–∏–ª–æ –Ω–µ–¥–∞–≤–Ω–µ–≥–æ –±–µ–∑—É–º–Ω—ã–∏ –Ω–µ—á–µ–≥–æ –∏–¥–∏–æ ##–∑–æ–≤–æ–º ##–∞–≤—à–µ–∏ –≤—Å–ø–æ–º–Ω–∏–ª ##—Ö–æ–≤–∞ —Å–≤–µ—Ä—Å—Ç —Å–≤–µ—Ä—Å—Ç —Ñ–∞—à–∏ –Ω–∞—Ç—è–∂–Ω—ã–µ –∫–ª–∞–¥ –∑–≤–µ–∑–¥–æ –ø—Ä–∏–µ–º–ª–µ–º –ø–æ–¥–≤–∏–≥–∏ –ø—Ä–æ–∏–≥—Ä–∞–ª –ª—é–±–∞—è –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –≤–≤—Å —Ç–∞–Ω—Ü–µ–≤ –≤–ª–∞—Å—Ç–µ–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏ ##—Ä–µ—Ü –Ω–∞—Ç—è–∂–Ω—ã–µ –∫–ª–∞–¥ –∑–≤–µ–∑–¥–æ –ø—Ä–∏–µ–º–ª–µ–º –ø–æ–¥–≤–∏–≥–∏ –¥—Ä–∞–≥–æ—Ü–µ–Ω ##—Ç–æ–≤–∏–¥ ##—â–µ–∏ ##—Ç–∏–µ toyota —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ_–æ'"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è\n",
        "\n",
        "eval_data_sample = next(iter(eval_dataloader))\n",
        "model.generate(eval_data_sample['input_ids'][:1].to(device), eval_data_sample['attention_mask'][:1].to(device), tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H2L-0BmZyu1"
      },
      "source": [
        "## –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ (1 –±–∞–ª–ª)\n",
        "\n",
        "<small> 0.25 –±–∞–ª–ª–∞ –∑–∞ –ø—Ä–æ—Å—Ç–µ–π—à–∏–π —Ä–∞–±–æ—á–∏–π —Ü–∏–∫–ª; </small>\n",
        "\n",
        "<small> +0.5 –±–∞–ª–ª–∞ –∑–∞ –≥—Ä–∞—Ñ–∏–∫–∏ –¥–ª—è –ª–æ—Å—Å–∞ –∏ –º–µ—Ç—Ä–∏–∫ –Ω–∞ —Ç—Ä–µ–π–Ω–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏.</small>\n",
        "\n",
        "–í –¥–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ –≤–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ **—Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ü–∏–∫–ª –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "us3xiacHBm-U"
      },
      "outputs": [],
      "source": [
        "# –ü—Ä–∏–º–µ—Ä –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –æ–¥–Ω–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏\n",
        "\n",
        "def train_step(model, input_ids, attention_mask, decoder_input_ids, optimizer, criterion):\n",
        "    \n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(input_ids, attention_mask, decoder_input_ids)\n",
        "    loss = criterion(outputs.view(-1, outputs.size(-1)), decoder_input_ids.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def plot_losses(train_losses, val_losses):\n",
        "    '''\n",
        "    Plot losses and metrics while training\n",
        "    - train_losses: sequence of train losses\n",
        "    - val_losses: sequence of validation losses\n",
        "    '''\n",
        "    clear_output()\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    axs[0].plot(range(1, len(train_losses) + 1), train_losses, label='train')\n",
        "    axs[0].plot(range(1, len(val_losses) + 1), val_losses, label='val')\n",
        "\n",
        "    if max(train_losses) / min(train_losses) > 10:\n",
        "        axs[0].set_yscale('log')\n",
        "\n",
        "    for ax in axs:\n",
        "        ax.set_xlabel('epoch')\n",
        "        ax.legend()\n",
        "\n",
        "    axs[0].set_ylabel('loss')\n",
        "    axs[1].set_ylabel('MSE')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch: 1:   0%|          | 0/305 [00:00<?, ?it/s]/Users/ivanyuminov/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "Epoch: 1:   3%|‚ñé         | 8/305 [03:18<2:02:59, 24.85s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[103], line 33\u001b[0m\n\u001b[1;32m     22\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m     23\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m input_ids,\n\u001b[1;32m     24\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask,\n\u001b[1;32m     25\u001b[0m     decoder_input_ids \u001b[38;5;241m=\u001b[39m shifted_labels\n\u001b[1;32m     26\u001b[0m     )\n\u001b[1;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(\n\u001b[1;32m     29\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, outputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)),\n\u001b[1;32m     30\u001b[0m     labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     31\u001b[0m     )\n\u001b[0;32m---> 33\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     35\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr= 1e-3)\n",
        "num_epochs = 1\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "eval_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for batch in tqdm(train_dataloader, desc= f'Epoch: {epoch + 1}'):\n",
        "        input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
        "\n",
        "        shifted_labels = torch.cat(\n",
        "            [labels[:, 1:], torch.full((labels.size(0), 1), tokenizer.pad_token_id).to(device)], \n",
        "            dim = 1\n",
        "            )\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids = input_ids,\n",
        "            attention_mask = attention_mask,\n",
        "            decoder_input_ids = shifted_labels\n",
        "            )\n",
        "        \n",
        "        loss = criterion(\n",
        "            outputs.view(-1, outputs.size(-1)),\n",
        "            labels.view(-1)\n",
        "            )\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    train_losses.append(running_loss / len(train_dataloader))\n",
        "    model.eval()\n",
        "    running_loss = 0\n",
        "    for batch in tqdm(eval_dataloader, desc= f'Epoch: {epoch + 1}'):\n",
        "        input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
        "\n",
        "        shifted_labels = torch.cat(\n",
        "            [labels[:, 1:], torch.full((labels.size(0), 1), tokenizer.pad_token_id).to(device)], \n",
        "            dim = 1\n",
        "            )\n",
        "        with torch.no_grad():\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids = input_ids,\n",
        "                attention_mask = attention_mask,\n",
        "                decoder_input_ids = shifted_labels\n",
        "                )\n",
        "            \n",
        "            loss = criterion(\n",
        "                outputs.view(-1, outputs.size(-1)),\n",
        "                labels.view(-1)\n",
        "                )\n",
        "\n",
        "            running_loss += loss.item()\n",
        "    \n",
        "    eval_losses.append(running_loss / len(eval_dataloader))\n",
        "    plot_losses(train_losses, eval_losses)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo01OhsoaacU"
      },
      "source": [
        "## –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ (1 –±–∞–ª–ª)\n",
        "\n",
        "<small>–ü–æ 0.33 –±–∞–ª–ª–∞ –∑–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é –∫–∞–∂–¥–æ–π –∏–∑ –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º—ã—Ö –º–µ—Ç—Ä–∏–∫</small>\n",
        "\n",
        "**–†–µ–∞–ª–∏–∑—É–π—Ç–µ —Ñ—É–Ω–∫–∏—Ü–∏—é –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏.**\n",
        "\n",
        "–î–æ–∫—É–º–µ—Ç–∞—Ü–∏—è –ø–æ –Ω–µ–∫–æ—Ç—Ä—ã–º –º–µ—Ç—Ä–∏–∫–∞–º:\n",
        " 1. [HuggingFace Rouge](https://huggingface.co/spaces/evaluate-metric/rouge)\n",
        " 2. [HuggingFace Bleu](https://huggingface.co/spaces/evaluate-metric/bleu)\n",
        " 3. [HuggingFace BERT Score](https://huggingface.co/spaces/evaluate-metric/bertscore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBNcGXt8aSJ2"
      },
      "outputs": [],
      "source": [
        "def compute_metrics():\n",
        "    #<YOUR CODE HERE>\n",
        "    pass\n",
        "\n",
        "def evaluation():\n",
        "    #<YOUR CODE HERE>\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ5GaAZ1chBu"
      },
      "source": [
        "## –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ (0.5 –±–∞–ª–ª–∞)\n",
        "**–û–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å, —Å–æ—Ö—Ä–∞–Ω–∏—Ç–µ –ª—É—á—à—É—é –≤–µ—Ä—Å–∏—é** (–º–µ—Ç–æ–¥ `.save_pretrained()` –æ–±—ä–µ–∫—Ç–∞ –∫–ª–∞—Å—Å–∞ AutoModel... –∏–ª–∏ `torch.save()`) **–∏ –¥–æ–±–∞–≤—å—Ç–µ –ø—Ä–∏–º–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏**. –£—á—Ç–∏—Ç–µ, —á—Ç–æ –µ—Å–ª–∏ –∏–∑–º–µ–Ω—è–ª—Å—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (–∞ –ª—É—á—à–µ –ø—Ä–æ—Å—Ç–æ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é), –µ–≥–æ —Ç–æ–∂–µ –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å. –ï—Å–ª–∏ –ø–ª–∞–Ω–∏—Ä—É–µ—Ç–µ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ\n",
        "\n",
        "–î–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ –∑–Ω–∞—á–µ–Ω–∏—è–º —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ –º–æ–∂–µ—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å ruT5-small –±–µ–∑ –¥–æ–æ–±—É—á–µ–Ω–∏—è. –ú—ã –Ω–∞–º–µ—Ä–µ–Ω–Ω–æ –¥–∞–µ–º –±–µ–π–∑–ª–∞–π–Ω –∏–º–µ–Ω–Ω–æ –≤ —Ç–∞–∫–æ–º –≤–∏–¥–µ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHu9RzbQcceV"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"YOUR MODEL\")\n",
        "summary = #<YOUR CODE HERE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbQH_vj6d2Ue"
      },
      "source": [
        "## –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–µ –∂–∞–¥–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –≤—ã–±–æ—Ä–∞ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ (4 –±–∞–ª–ª–∞)\n",
        "–í—Å–µ–≥–¥–∞ –ª–∏ –≤—ã–±–æ—Ä –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ ‚Äì¬†—ç—Ç–æ –ª—É—á—à–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞?\n",
        "\n",
        "<details>\n",
        "    <summary>–°–ø–æ–π–ª–µ—Ä</summary>\n",
        "    <p>–ù–µ—Ç</p>\n",
        "</details>\n",
        "\n",
        "**–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞:**\n",
        "\n",
        "| Strategy | Description | Pros & Cons |\n",
        "| --- | --- | --- |\n",
        "| Greedy Search | Chooses the word with the highest probability as the next word in the sequence. | **Pros:** Simple and fast. <br><br/> **Cons:** Can lead to repetitive and incoherent text. |\n",
        "| Sampling with Temperature | Introduces randomness in the word selection. A higher temperature leads to more randomness. | **Pros:** Allows exploration and diverse output. <br><br/> **Cons:** Higher temperatures can lead to nonsensical outputs. |\n",
        "| Nucleus Sampling (Top-p Sampling) | Selects the next word from a truncated vocabulary, the \"nucleus\" of words <br/> that have a cumulative probability exceeding a pre-specified threshold (p). | **Pros:** Balances diversity and quality. <br><br/> **Cons:** Setting an optimal 'p' can be tricky. |\n",
        "| Beam Search | Explores multiple hypotheses (sequences of words) at each step, and keeps <br/> the 'k' most likely, where 'k' is the beam width. | **Pros:** Produces more reliable results than greedy search. <br><br/> **Cons:** Can lack diversity and lead to generic responses. |\n",
        "| Top-k Sampling | Randomly selects the next word from the top 'k' words with the highest probabilities. | **Pros:** Introduces randomness, increasing output diversity. <br><br/> **Cons:** Random selection can sometimes lead to less coherent outputs. |\n",
        "| Length Normalization | Prevents the model from favoring shorter sequences by dividing the log probabilities <br/> by the sequence length raised to some power. | **Pros:** Makes longer and potentially more informative sequences more likely. <br><br/> **Cons:** Tuning the normalization factor can be difficult. |\n",
        "| Stochastic Beam Search | Introduces randomness into the selection process of the 'k' hypotheses in beam search. | **Pros:** Increases diversity in the generated text. <br><br/> **Cons:** The trade-off between diversity and quality can be tricky to manage. |\n",
        "| Decoding with Minimum Bayes Risk (MBR) | Chooses the hypothesis (out of many) that minimizes expected loss under a loss function. | **Pros:** Optimizes the output according to a specific loss function. <br><br/> **Cons:** Computationally more complex and requires a good loss function. |\n",
        "\n",
        "–°—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ–∫—É–º–µ—Ç–∞—Ü–∏—é:\n",
        "- [reference for `AutoModelForCausalLM.generate()`](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationMixin.generate)\n",
        "- [reference for `AutoTokenizer.decode()`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.decode)\n",
        "- Huggingface [docs on generation strategies](https://huggingface.co/docs/transformers/generation_strategies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQF4Vc3msKpF"
      },
      "source": [
        "**1. –î–æ–ø–æ–ª–Ω–∏—Ç–µ –º–µ—Ç–æ–¥ `generate` –≤ –º–æ–¥–µ–ª–∏, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∞—Ç—å —Ç–æ–ø-k —Å–∞–º—ã—Ö –≤–µ—Ä–æ—è—Ç–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∞ –∏ –∏—Ö \"–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏\"** (1 –±–∞–ª–ª).   \n",
        "\n",
        "**2. –†–µ–∞–ª–∏–∑—É–π—Ç–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é Nucleus Sampling –≤ –º–µ—Ç–æ–¥–µ `generate`** (1 –±–∞–ª–ª)\n",
        "\n",
        "**3. –†–µ–∞–ª–∏–∑—É–π—Ç–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é Beam Search** (2 –±–∞–ª–ª–∞)\n",
        "\n",
        "–ü–æ–ª—É—á–∏–ª–æ—Å—å –ª–∏ —É–ª—É—á—à–∏—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRfAEfP5kHcc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbiksVMOOvO8"
      },
      "source": [
        "## –ü–æ—Å–ª–µ–≤–∫—É—Å–∏–µ (0 –±–∞–ª–ª–æ–≤)\n",
        "\n",
        "–ï—Å–ª–∏ —ç—Ç–∞ –¥–æ–º–∞—à–Ω—è—è —Ä–∞–±–æ—Ç–∞ –ø–æ–∫–∞–∑–∞–ª–∞—Å—å –≤–∞–º –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–æ–π, –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º –ø—Ä–æ–≤–µ—Å—Ç–∏ —Å–ª–µ–¥—É—é—â–∏–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç:\n",
        "\n",
        "- –æ—Ç –∏–º–µ—é—â–µ–π—Å—è –º–æ–¥–µ–ª–∏ \"–æ—Ç–∫—É—Å–∏—Ç—å\" —Ç–æ–ª—å–∫–æ –¥–µ–∫–æ–¥–µ—Ä–Ω—É—é —á–∞—Å—Ç—å (–æ—Ç–∫—É—Å–∏—Ç—å —Ç–∞–∫–∂–µ –º–æ–∂–Ω–æ –æ—Ç ruT5-small);\n",
        "- –Ω–µ–º–Ω–æ–≥–æ –¥–æ–æ–±—É—á–∏—Ç—å (—á—Ç–æ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è, –ø–æ –≤–∫—É—Å—É);\n",
        "- –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º –∏ \"–≥–ª–∞–∑–∞–º–∏\";\n",
        "- —Å—Ä–∞–≤–Ω–∏—Ç—å –ø–æ–ª—É—á–µ–Ω–Ω–æ–µ —Å Encoder-Decoder –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π;\n",
        "- –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å \"–î–∞–µ—Ç –ª–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ Encoder-Decoder –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –±—É—Å—Ç –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –∏–ª–∏ —ç—Ç–æ –Ω–µ–∫–æ—Ç–æ—Ä—ã–π overkill?\" (–±–∞–∑–æ–≤–æ, –æ—Ç–≤–µ—Ç –ª–µ–∂–∏—Ç –Ω–∞ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏ üò∏)\n",
        "\n",
        "–ï—â—ë –±–æ–ª–µ–µ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –º–æ–∂–Ω–æ:\n",
        "- –ø–æ—á–∏—Ç–∞—Ç—å –ø—Ä–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ Encoder-only –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–º–∏ —Ä–µ—à–µ–Ω–∏—è–º–∏ (BERT, e.g.)\n",
        "- —Å—Ä–∞–≤–Ω–∏—Ç—å —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Ç–æ–ª—å–∫–æ Decoder'–æ–º –∏ both Encoder-Decoder'–æ–º;\n",
        "- –≤ —Ç.—á. –ø–æ–¥–æ–±—Ä–∞—Ç—å —á–∏—Å–ª–æ –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —á—Ç–æ–± –æ–Ω–æ –±—ã–ª–æ –ø—Ä–∏–º–µ—Ä–Ω–æ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–Ω—Å—Ç–∞–Ω—Å–∞ –º–æ–¥–µ–ª–µ–π (–∏—Ö, –∏–Ω—Å—Ç–∞–Ω—Å–æ–≤, –±—É–¥–µ—Ç 3 -- —Ç–æ–ª—å–∫–æ —ç–Ω–∫–æ–¥–µ—Ä, —Ç–æ–ª—å–∫–æ –¥–µ–∫–æ–¥–µ—Ä –∏ —ç–Ω–∫–æ–¥–µ—Ä-–¥–µ–∫–æ–¥–µ—Ä).\n",
        "\n",
        "*–í–æ–æ–±—â–µ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è —Å–ª–µ–¥—É–µ—Ç –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ: \"–¢–æ–ª—å–∫–æ —ç–Ω–∫–æ–¥–µ—Ä–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã (BERT, e.g.) —Ö–æ—Ä–æ—à–∏ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ (–ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–µ–¥–¥–∏–Ω–≥–æ–≤), –ª–∏—à—å –¥–µ–∫–æ–¥–µ—Ä–Ω—ã–µ (GPT, –Ω–∞–ø—Ä–∏–º–µ—Ä) -- –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, —ç–Ω–∫–æ–¥–µ—Ä-–¥–µ–∫–æ–¥–µ—Ä–Ω—ã–µ (—Å–∫–∞–∂–µ–º, T5) -- –¥–ª—è –æ–±–µ–∏—Ö –∑–∞–¥–∞—á\"*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZM1xLliO1QM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06437165ba564bff9e29aa53f4c0df5b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09b11d6c6eae4979a1c8cd42e32730ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab3d20b0e457431dbe24120bb4becede",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_68b44cb7bddd4a2f950db1a9c00ce066",
            "value": "Map:‚Äá100%"
          }
        },
        "2665d63c206a45d88fada74bc984771e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40c29f2dde174a3c8442d9b86a4e3fe9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "665428d410664f94babcd067b879ce2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68b44cb7bddd4a2f950db1a9c00ce066": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "911fd70fda12476ab2b788433d6ff83f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2665d63c206a45d88fada74bc984771e",
            "max": 3048,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c0ddd3783ec047569c2024e461d5ad0f",
            "value": 3048
          }
        },
        "a8e5203a02b845a29f57ca545c50c5d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_09b11d6c6eae4979a1c8cd42e32730ae",
              "IPY_MODEL_911fd70fda12476ab2b788433d6ff83f",
              "IPY_MODEL_bcfc58a3f90745449c3f559aa5ab999a"
            ],
            "layout": "IPY_MODEL_40c29f2dde174a3c8442d9b86a4e3fe9"
          }
        },
        "ab3d20b0e457431dbe24120bb4becede": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcfc58a3f90745449c3f559aa5ab999a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06437165ba564bff9e29aa53f4c0df5b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_665428d410664f94babcd067b879ce2e",
            "value": "‚Äá3048/3048‚Äá[00:13&lt;00:00,‚Äá206.31‚Äáexamples/s]"
          }
        },
        "c0ddd3783ec047569c2024e461d5ad0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
